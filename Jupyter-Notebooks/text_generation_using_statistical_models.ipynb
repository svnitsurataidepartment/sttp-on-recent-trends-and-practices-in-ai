{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4df9873b-a4b6-407b-b2f5-301afda57e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation example using Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a59affa8-e7fc-4ab9-906b-12151cdc4b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "43a4d000-6da0-4522-9611-43288f828e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b797c25a-a8d8-472b-b230-8d6b670592ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for different sentences\n",
    "sentence = \"John is a good boy.\"\n",
    "sentence = \"John runs fast.\"\n",
    "# sentence = \"The quick brown fox jumps over the lazy dog.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df7b5127-87cf-492c-921a-525c20d29e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea4deefd-4d9f-4d75-91a3-9e86570a5ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('all')\n",
    "# wn.synonyms('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "719e46e4-1bd8-4625-845d-f33675ac0be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('John', 'NNP'), ('runs', 'VBZ'), ('fast', 'RB'), ('.', '.')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(sentence)\n",
    "pos_tags = pos_tag(tokens)\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b375d410-9ecb-45c4-9c45-0f6164809a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fcde8f13-fe15-4039-9916-03ed84d73658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "from random import choice\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e67f15bf-0fe8-4a9a-9cdb-eb7714013b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John endure fast .\n"
     ]
    }
   ],
   "source": [
    "lexical_category = ''\n",
    "new_words = []\n",
    "for token, pos in pos_tags:\n",
    "    if search('^VB.*$', pos):\n",
    "        lexical_category = 'v'\n",
    "    elif search('^NN[S]?$', pos):\n",
    "        lexical_category = 'n'\n",
    "    elif search('^JJ$', pos):\n",
    "        lexical_category = 'a'\n",
    "    elif search('^RB$', pos):\n",
    "        lexical_category = 'r'\n",
    "    if lexical_category:\n",
    "        synonyms = []\n",
    "        stemmed = stemmer.stem(token)\n",
    "        if not wn.synsets(stemmed):\n",
    "            stemmed = token\n",
    "        for syn in wn.synsets(stemmed):\n",
    "            for lemma in syn.lemmas():\n",
    "                if lexical_category in ['a', 'r']:\n",
    "                    antonyms = []\n",
    "                    # replace with antonym if it exists else replace a synonym\n",
    "                    for ant in lemma.antonyms():\n",
    "                        if ant.name() != stemmed.lower() and '_' not in ant.name() and ant.name() not in antonyms:\n",
    "                            antonyms.append(ant.name())\n",
    "                    if antonyms:\n",
    "                        antonym = choice(antonyms)\n",
    "                    else:\n",
    "                        antonym = lemma.name()\n",
    "                    synonyms.append(antonym)\n",
    "                elif lexical_category not in ['a', 'r'] and lemma.name() != stemmed.lower() and '_' not in lemma.name() and lemma.name() not in synonyms:\n",
    "                    synonyms.append(lemma.name())\n",
    "            if synonyms:\n",
    "                new_word = choice(synonyms)\n",
    "            else:\n",
    "                new_word = token\n",
    "    else:\n",
    "        new_word = token\n",
    "    new_words.append(new_word)\n",
    "    lexical_category = ''\n",
    "print(' '.join(new_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f0f77e9f-f415-4435-8123-51a641f49f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # movie_review dataset\n",
    "from nltk.corpus import movie_reviews\n",
    "# Prints file ids of positive reviews\n",
    "sample_pos_files = movie_reviews.fileids('pos')[: 500]\n",
    "# Prints file ids of negative reviews.\n",
    "sample_neg_files = movie_reviews.fileids('neg')[: 500]\n",
    "all_text_reviews = ''\n",
    "for sample_file in sample_pos_files + sample_neg_files:\n",
    "    text = movie_reviews.raw(sample_file)\n",
    "    # preprocess the data\n",
    "    text = text.replace('\\n', '')\n",
    "    all_text_reviews += text + ' '\n",
    "all_text_reviews = all_text_reviews.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2c9b4fcf-8728-46f2-8443-b3af8e170a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11793333 2135242\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "# Prints file ids of gutenberg\n",
    "sample_fileids = gutenberg.fileids()[: 100]\n",
    "all_text_gutenberg = ''\n",
    "for sample_file in sample_fileids:\n",
    "    text = gutenberg.raw(sample_file)\n",
    "    # preprocess the data\n",
    "    text = text.replace('\\n', ' ')\n",
    "    all_text_gutenberg += text + ' '\n",
    "all_text_gutenberg = all_text_gutenberg.strip()\n",
    "print(len(all_text_gutenberg), len(all_text_gutenberg.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "21af3e42-8ad8-4fbc-8172-61a7849459c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank\n",
    "# Prints file ids of WSJ in treebank\n",
    "sample_fileids = treebank.fileids()[: 1000]\n",
    "all_text_treebank = ''\n",
    "for sample_file in sample_fileids:\n",
    "    text = treebank.raw(sample_file)\n",
    "    # preprocess the data\n",
    "    text = text.replace('\\n', ' ')\n",
    "    all_text_treebank += text + ' '\n",
    "all_text_treebank = all_text_treebank.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7603c3b-4db7-4f24-84b6-ea2d9154884b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "\n",
    "# --- 1. Sample Corpus ---\n",
    "# For a real application, you would load a much larger text file.\n",
    "# For this example, we'll use a small, simple corpus.\n",
    "# sample_corpus = \"\"\"\n",
    "# Natural language processing (NLP) is a field of artificial intelligence, \n",
    "# computer science, and computational linguistics concerned with the \n",
    "# interactions between computers and human (natural) languages. \n",
    "# As such, NLP is about programming computers to process and analyze \n",
    "# large amounts of natural language data. Challenges in natural language \n",
    "# processing often involve speech recognition, natural language understanding, \n",
    "# and natural language generation.\n",
    "# \"\"\"\n",
    "# select the corpus of your choice\n",
    "corpus = all_text_gutenberg\n",
    "# --- 2. Preprocessing the Corpus ---\n",
    "# Tokenization: Breaking text into words/tokens\n",
    "# Lowercasing: Normalizing words to avoid treating \"The\" and \"the\" as different.\n",
    "# Punctuation removal (optional, but good for basic models)\n",
    "\n",
    "def preprocess_text(text, flag='bi'):\n",
    "    # Tokenize the text into words\n",
    "    all_words = []\n",
    "    for sentence in nltk.sent_tokenize(text):\n",
    "        tokens = nltk.word_tokenize(sentence.lower())\n",
    "        # Filter out non-alphabetic tokens (basic punctuation removal)\n",
    "        # You might want more sophisticated handling for numbers, contractions etc.\n",
    "        words = [word for word in tokens if word.isalpha()]\n",
    "        if flag == 'bi':\n",
    "            words = ['<S>'] + words + ['</S>']\n",
    "        else:\n",
    "            words = ['<S>', '<S>'] + words + ['</S>', '</S>']\n",
    "        all_words += words\n",
    "    return all_words\n",
    "\n",
    "processed_words_bigrams = preprocess_text(corpus, 'bi')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f15fa7b0-fbd4-4a56-a385-1bda1031c146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2283281 ['<S>', 'emma', 'by', 'jane', 'austen']\n"
     ]
    }
   ],
   "source": [
    "print(len(processed_words_bigrams), processed_words_bigrams[: 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3497e2b-ebff-40bf-8f84-abd37f55b6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unigram_count_dict(words):\n",
    "    \"\"\"Create unigram count dict.\"\"\"\n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ca051a97-91ab-48c3-9646-fe08929ac2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40794\n"
     ]
    }
   ],
   "source": [
    "unigram_counts = create_unigram_count_dict(processed_words_bigrams)\n",
    "print(len(unigram_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "708d4743-9fa8-4175-bd47-909ff418a7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pie bedtime baptised registred getting erech thronged riuer unweighed peniel went campaign collapsing hostesse uncrossed nickname ignored gaal controls qualifies\n",
      "ripen mercy clipp merari cruised quietus tubercled rallying glum candor aladdin octagon arba tartar japhet learning ideals level careering belch\n",
      "spent enquirest blossomes iuggel linen refraction mattithiah trick carpenter molest meekin healthier snoozing hepherites instructions timed difficulties gezrites pecking superinduced\n"
     ]
    }
   ],
   "source": [
    "# Sentence creation using unigram model\n",
    "total_unigrams = sum(unigram_counts.values())\n",
    "# Using Lidstone smoothing\n",
    "alpha = 0.5\n",
    "vocab_size = len(unigram_counts)\n",
    "unigram_prob = {word: unigram_counts[word] + alpha / total_unigrams + vocab_size for word in unigram_counts}\n",
    "# random sentence generation with fixed no of words\n",
    "sentence_length = 20\n",
    "enumerated_dict = dict(enumerate(unigram_prob.keys()))\n",
    "total_indexes = range(len(enumerated_dict))\n",
    "for i in range(3):\n",
    "    words = []\n",
    "    for j in range(sentence_length):\n",
    "        random_word = enumerated_dict[choice(total_indexes)]\n",
    "        if random_word == '</S>':\n",
    "            break\n",
    "        else:\n",
    "            words.append(random_word)\n",
    "    # generates garbage\n",
    "    print(' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5c09611b-f64e-4c33-939c-d9f573f262e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40794\n"
     ]
    }
   ],
   "source": [
    "# determine the vocabulary size\n",
    "vocab_size = len(unigram_counts)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4710f868-54b0-49de-9f1b-a5678ffce9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Creating bigrams\n",
    "def create_bigrams(words):\n",
    "    # Add start and end tokens.\n",
    "    # '<S>' for start of sentence, '</S>' for end of sentence.\n",
    "    # For a simple corpus like this, we'll just treat the whole thing as one \"sentence\" for simplicity,\n",
    "    # or you can split by actual sentences first.\n",
    "    # For now, let's just use a single start/end token for the whole text for simplicity of demonstration.\n",
    "    # For multiple sentences, you'd apply this per sentence after sentence tokenization.\n",
    "    \n",
    "    # Example for a full text with sentences:\n",
    "    # sentences = nltk.sent_tokenize(text.lower())\n",
    "    # all_bigrams = []\n",
    "    # for sent in sentences:\n",
    "    #     tokens = [word for word in nltk.word_tokenize(sent) if word.isalpha()]\n",
    "    #     padded_tokens = ['<S>'] + tokens + ['</S>']\n",
    "    #     for i in range(len(padded_tokens) - 1):\n",
    "    #         all_bigrams.append(tuple(padded_tokens[i:i + 2]))\n",
    "    # return all_bigrams\n",
    "\n",
    "    # Simpler approach for this small text example:\n",
    "    # padded_words = ['<S>'] + words + ['</S>'] # Pad with one start/end token\n",
    "    bigrams = []\n",
    "    for i in range(len(words) - 1):\n",
    "        bigrams.append(tuple(words[i: i + 2]))\n",
    "    return bigrams\n",
    "\n",
    "bigrams = create_bigrams(processed_words_bigrams)\n",
    "print('Done')\n",
    "\n",
    "def build_bigram_model(bigrams, vocab_size=0, smoothing=False, alpha=0):\n",
    "    bigram_counts = {}\n",
    "\n",
    "    for w1, w2 in bigrams:\n",
    "        bigram_counts.setdefault(w1, {})\n",
    "        bigram_counts[w1][w2] = bigram_counts[w1].get(w2, 0) + 1\n",
    "    # Convert counts to probabilities (smoothed with Lidstone smoothing if needed, but omitted for simplicity here)\n",
    "    # For each w1 unigram, calculate P(w2 | w1)\n",
    "    bigram_probabilities = {}\n",
    "    for w1 in bigram_counts:\n",
    "        total_unigram_count = sum(bigram_counts[w1].values())\n",
    "        for w2, count in bigram_counts[w1].items():\n",
    "            bigram_probabilities.setdefault(w1, {})\n",
    "            if not smoothing:\n",
    "                bigram_probabilities[w1][w2] = count / total_unigram_count\n",
    "            else:\n",
    "                bigram_probabilities[w1][w2] = count + alpha / total_unigram_count + alpha * vocab_size\n",
    "            \n",
    "    return bigram_probabilities, bigram_counts # Also return counts for debugging/analysis\n",
    "\n",
    "# Probabilities with smoothing\n",
    "alpha = 0.5\n",
    "smoothing = True\n",
    "bigram_model, bigram_counts_raw = build_bigram_model(bigrams, vocab_size, smoothing, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "45eb6695-6009-47a4-aef9-6531405ceccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generated Text ---\n",
      "Log Probability of the generated text = 19.846536241303905\n",
      "Generated 1: Ex.\n",
      "Log Probability of the generated text = 198.46588717972807\n",
      "Generated 2: Jump into stillness here inquiring tireless and scarlet embrace some dosen or ours be righteousness godliness looking little brothers unasked.\n",
      "Log Probability of the generated text = 29.77670959763559\n",
      "Generated 3: Evidently about.\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "def generate_text_using_bigram_model(model, num_words=20, start_sequence=('<S>',)):\n",
    "    generated_text = list(start_sequence)\n",
    "    # find the log prob score\n",
    "    log_prob_score = 0\n",
    "    for _ in range(num_words):\n",
    "        current_unigram = generated_text[-1]\n",
    "        # Get possible next words and their probabilities\n",
    "        possible_next_words_probs = model.get(current_unigram)\n",
    "        if not possible_next_words_probs:\n",
    "            # If no continuation found, stop or restart with a common bigram\n",
    "            # For simplicity, we'll stop here. In a more robust model, you'd handle this.\n",
    "            # print(f\"\\nNo known continuation for {current_bigram}. Stopping generation.\")\n",
    "            break\n",
    "            \n",
    "        # Select the next word based on probabilities\n",
    "        words = list(possible_next_words_probs.keys())\n",
    "        probabilities = list(possible_next_words_probs.values())\n",
    "        \n",
    "        next_word = random.choices(words, weights=probabilities, k=1)[0]\n",
    "        log_prob_score += log(model[current_unigram][next_word])\n",
    "        generated_text.append(next_word)\n",
    "        \n",
    "        # Stop if we generate an end-of-sentence token\n",
    "        if next_word == '</S>':\n",
    "            break\n",
    "            \n",
    "    # Clean up the generated text (remove start/end tokens, join words)\n",
    "    clean_text_tokens = [word for word in generated_text if word not in ['<S>', '</S>']]\n",
    "    print('Log Probability of the generated text =', log_prob_score)\n",
    "    return \" \".join(clean_text_tokens)\n",
    "\n",
    "# --- 6. Generate Text ---\n",
    "print(f\"\\n--- Generated Text ---\")\n",
    "for i in range(3): # Generate a few sentences\n",
    "    generated_sentence = generate_text_using_bigram_model(bigram_model, num_words=20)\n",
    "    print(f\"Generated {i+1}: {generated_sentence.capitalize()}.\") # Capitalize for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cc2622f7-a407-4e81-a2f8-afb0b7ab131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_words_trigrams = preprocess_text(corpus, 'tri')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bf33ba64-793a-4149-8b75-ac9394478786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generated Text ---\n",
      "Log Probability of the generated text = 9.923249329324735\n",
      "Generated 1: Decius.\n",
      "Log Probability of the generated text = 9.923216647717123\n",
      "Generated 2: Zoned by those faggots.\n",
      "Log Probability of the generated text = 9.923216647717123\n",
      "Generated 3: Book xvii.\n",
      "Log Probability of the generated text = 9.923486499734262\n",
      "Generated 4: Revenge at first as other people the difference that the willoughbys nothing of marianne till dinner time she knew not.\n",
      "Log Probability of the generated text = 9.923216647717123\n",
      "Generated 5: Larboard boat there save for civilities till they shine yea they are vain for the best outline pictures are in.\n",
      "\n",
      "--- Generated Text With Most Probable Next Word---\n",
      "Log Probability of the generated text = 9.957549513979904\n",
      "Generated 1: And the lord.\n",
      "Log Probability of the generated text = 9.957549513979904\n",
      "Generated 2: And the lord.\n",
      "Log Probability of the generated text = 9.957549513979904\n",
      "Generated 3: And the lord.\n",
      "Log Probability of the generated text = 9.957549513979904\n",
      "Generated 4: And the lord.\n",
      "Log Probability of the generated text = 9.957549513979904\n",
      "Generated 5: And the lord.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Creating Trigrams ---\n",
    "# A trigram is a sequence of three consecutive words.\n",
    "# We'll also add special start/end tokens to handle sentence boundaries\n",
    "# and allow generation to start and end naturally.\n",
    "# Smoothing can also be incorporated\n",
    "def create_trigrams(words):\n",
    "    # Add start and end tokens.\n",
    "    # '<S>' for start of sentence, '</S>' for end of sentence.\n",
    "    # For a simple corpus like this, we'll just treat the whole thing as one \"sentence\" for simplicity,\n",
    "    # or you can split by actual sentences first.\n",
    "    # For now, let's just use a single start/end token for the whole text for simplicity of demonstration.\n",
    "    # For multiple sentences, you'd apply this per sentence after sentence tokenization.\n",
    "    \n",
    "    # Example for a full text with sentences:\n",
    "    # sentences = nltk.sent_tokenize(text.lower())\n",
    "    # all_trigrams = []\n",
    "    # for sent in sentences:\n",
    "    #     tokens = [word for word in nltk.word_tokenize(sent) if word.isalpha()]\n",
    "    #     padded_tokens = ['<S>', '<S>'] + tokens + ['</S>']\n",
    "    #     for i in range(len(padded_tokens) - 2):\n",
    "    #         all_trigrams.append(tuple(padded_tokens[i:i+3]))\n",
    "    # return all_trigrams\n",
    "\n",
    "    # Simpler approach for this small text example:\n",
    "    # padded_words = ['<S>', '<S>'] + words + ['</S>', '</S>'] # Pad with two start/end tokens\n",
    "    trigrams = []\n",
    "    for i in range(len(words) - 2):\n",
    "        trigrams.append(tuple(words[i: i + 3]))\n",
    "    return trigrams\n",
    "\n",
    "trigrams = create_trigrams(processed_words_trigrams)\n",
    "# print(f\"\\nSample Trigrams (first 5): {trigrams[:5]}\")\n",
    "# print(f\"Sample Trigrams (last 5): {trigrams[-5:]}\")\n",
    "\n",
    "\n",
    "# --- 4. Building the Trigram Probability Model ---\n",
    "# We need:\n",
    "# 1. Counts of (word1, word2, word3) - trigram_counts\n",
    "# 2. Counts of (word1, word2) - bigram_counts (for the denominator)\n",
    "\n",
    "def build_trigram_model(trigrams, vocab_size=0, smoothing=False, alpha=0):\n",
    "    trigram_counts = {}\n",
    "\n",
    "    for w1, w2, w3 in trigrams:\n",
    "        trigram_counts.setdefault((w1, w2), {})\n",
    "        trigram_counts[(w1, w2)][w3] = trigram_counts[(w1, w2)].get(w3, 0) + 1\n",
    "    # return trigram_counts, bigram_counts\n",
    "    # Convert counts to probabilities (smoothed with Lidstone smoothing if needed, but omitted for simplicity here)\n",
    "    # For each (w1, w2) bigram, calculate P(w3 | w1, w2)\n",
    "    trigram_probabilities = {}\n",
    "    for w1, w2 in trigram_counts:\n",
    "        total_bigram_count = sum(trigram_counts[(w1, w2)].values())\n",
    "        for w3, count in trigram_counts[(w1, w2)].items():\n",
    "            trigram_probabilities.setdefault((w1, w2), {})\n",
    "            trigram_probabilities[(w1, w2)][w3] = count + alpha / total_bigram_count + alpha * vocab_size\n",
    "            \n",
    "    return trigram_probabilities, trigram_counts # Also return counts for debugging/analysis\n",
    "\n",
    "# Probabilities with smoothing\n",
    "alpha = 0.5\n",
    "smoothing = True\n",
    "trigram_model, trigram_counts_raw = build_trigram_model(trigrams, vocab_size, smoothing, alpha)\n",
    "\n",
    "# Example: Probability of 'processing' after ('language', 'natural')\n",
    "# print(f\"\\nProbability of 'processing' after ('language', 'natural'): {trigram_model[('language', 'natural')].get('processing', 0)}\")\n",
    "# print(f\"Count of ('language', 'natural', 'processing'): {trigram_counts_raw[('language', 'natural')]['processing']}\")\n",
    "# print(f\"Sum of all words after ('language', 'natural'): {sum(trigram_counts_raw[('language', 'natural')].values())}\")\n",
    "\n",
    "\n",
    "# --- 5. Text Generation Function ---\n",
    "# Start with '<S>', '<S>' and iteratively predict the next word.\n",
    "\n",
    "def generate_text_using_trigram_model(model, num_words=20, start_sequence=('<S>', '<S>')):\n",
    "    generated_text = list(start_sequence)\n",
    "    \n",
    "    for _ in range(num_words):\n",
    "        current_bigram = tuple(generated_text[-2:])\n",
    "        log_prob_score = 0\n",
    "        # Get possible next words and their probabilities\n",
    "        possible_next_words_probs = model.get(current_bigram)\n",
    "        \n",
    "        if not possible_next_words_probs:\n",
    "            # If no continuation found, stop or restart with a common bigram\n",
    "            # For simplicity, we'll stop here. In a more robust model, you'd handle this.\n",
    "            # print(f\"\\nNo known continuation for {current_bigram}. Stopping generation.\")\n",
    "            break\n",
    "            \n",
    "        # Select the next word based on probabilities\n",
    "        words = list(possible_next_words_probs.keys())\n",
    "        probabilities = list(possible_next_words_probs.values())\n",
    "        \n",
    "        next_word = random.choices(words, weights=probabilities, k=1)[0]\n",
    "        generated_text.append(next_word)\n",
    "        log_prob_score += log(model[current_bigram][next_word])\n",
    "        # Stop if we generate an end-of-sentence token\n",
    "        if next_word == '</S>':\n",
    "            break\n",
    "            \n",
    "    # Clean up the generated text (remove start/end tokens, join words)\n",
    "    clean_text_tokens = [word for word in generated_text if word not in ['<S>', '</S>']]\n",
    "    print('Log Probability of the generated text =', log_prob_score)\n",
    "    return \" \".join(clean_text_tokens)\n",
    "\n",
    "# --- 6. Generate Text ---\n",
    "print(f\"\\n--- Generated Text ---\")\n",
    "for i in range(5): # Generate a few sentences\n",
    "    generated_sentence = generate_text_using_trigram_model(trigram_model, num_words=20)\n",
    "    print(f\"Generated {i + 1}: {generated_sentence.capitalize()}.\") # Capitalize for better readability\n",
    "\n",
    "import numpy as np\n",
    "def generate_text_using_trigram_model_with_most_probable_next_word(model, num_words=20, start_sequence=('<S>', '<S>')):\n",
    "    generated_text = list(start_sequence)\n",
    "    \n",
    "    for _ in range(num_words):\n",
    "        current_bigram = tuple(generated_text[-2:])\n",
    "        log_prob_score = 0\n",
    "        # Get possible next words and their probabilities\n",
    "        possible_next_words_probs = model.get(current_bigram)\n",
    "        \n",
    "        if not possible_next_words_probs:\n",
    "            # If no continuation found, stop or restart with a common bigram\n",
    "            # For simplicity, we'll stop here. In a more robust model, you'd handle this.\n",
    "            # print(f\"\\nNo known continuation for {current_bigram}. Stopping generation.\")\n",
    "            break\n",
    "            \n",
    "        # Select the next word based on probabilities\n",
    "        words = list(possible_next_words_probs.keys())\n",
    "        probabilities = list(possible_next_words_probs.values())\n",
    "        max_prob_index = np.argmax(np.array(probabilities))\n",
    "        next_word = words[max_prob_index]\n",
    "        generated_text.append(next_word)\n",
    "        log_prob_score += log(model[current_bigram][next_word])\n",
    "        # Stop if we generate an end-of-sentence token\n",
    "        if next_word == '</S>':\n",
    "            break\n",
    "            \n",
    "    # Clean up the generated text (remove start/end tokens, join words)\n",
    "    clean_text_tokens = [word for word in generated_text if word not in ['<S>', '</S>']]\n",
    "    print('Log Probability of the generated text =', log_prob_score)\n",
    "    return \" \".join(clean_text_tokens)\n",
    "\n",
    "# --- 6. Generate Text with greedy method---\n",
    "print(f\"\\n--- Generated Text With Most Probable Next Word---\")\n",
    "# In this greedy method you will get the same sentence every time, so the above random sampling or beam search is preferred\n",
    "for i in range(5): # Generate a few sentences\n",
    "    generated_sentence = generate_text_using_trigram_model_with_most_probable_next_word(trigram_model, num_words=20)\n",
    "    print(f\"Generated {i + 1}: {generated_sentence.capitalize()}.\") # Capitalize for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e7e5f714-eee9-44d6-af2a-b99941cd73c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generated Text ---\n",
      "Generated 1: Responsive to our father which art a man in whom we think she will think this that is a very.\n",
      "Generated 2: Urge and spur of every man in whom we think she will think this that is a thing that is.\n",
      "Generated 3: Tiny green leaves of her being settled there had of him as she did so with some anxiety that there.\n",
      "Generated 4: Italian music behind me sank back and saw and listened as animals listen for any change for heaven to give.\n",
      "Generated 5: Disguised as one can only recollect the fate of mother ocean it had never thought edward so i did not.\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Text Generation Function with beam search---\n",
    "# Start with '<S>', '<S>' and iteratively predict the next word.\n",
    "from random import sample\n",
    "def generate_text_with_beam_search(model, num_words=20, start_sequence=('<S>', '<S>'), beam_size=1):\n",
    "    # generated_text = list(start_sequence)\n",
    "    beam = [(0., list(start_sequence))]\n",
    "    for _ in range(num_words):\n",
    "        candidates = []\n",
    "        # Early stopping if all sequences have generated </S>\n",
    "        if all(seq[-1] == '</S>' for _, seq in beam):\n",
    "            break\n",
    "        for log_prob, seq in beam:\n",
    "            current_seq = seq[-2:]\n",
    "            # if the last token of the current sequence is </S>, then don't expand it\n",
    "            if current_seq[-1] == '</S>':\n",
    "                continue\n",
    "            # Get possible next words and their probabilities\n",
    "            possible_next_words_probs = model.get(tuple(current_seq))\n",
    "            if not possible_next_words_probs:\n",
    "                # If no continuation found, stop or restart with a common bigram\n",
    "                # For simplicity, we'll stop here. In a more robust model, you'd handle this.\n",
    "                # print(f\"\\nNo known continuation for {current_bigram}. Stopping generation.\")\n",
    "                break\n",
    "                \n",
    "            # Select all the candidates and evaluate their log probs\n",
    "            words = list(possible_next_words_probs.keys())\n",
    "            probabilities = list(possible_next_words_probs.values())\n",
    "            for word, prob in zip(words, probabilities):\n",
    "                new_log_prob = log_prob + log(prob)\n",
    "                new_seq = seq + [word]\n",
    "                # Stop if we generate an end-of-sentence token\n",
    "                candidates.append((new_log_prob, new_seq))\n",
    "            candidates.sort(key=lambda x: x[0], reverse=True)\n",
    "            # For the 1st word sample all words, not the best probability ones\n",
    "            if _ == 0:\n",
    "                beam = sample(candidates, beam_size)\n",
    "            else:\n",
    "                beam = candidates[: beam_size]\n",
    "            # beam = candidates[: beam_size]\n",
    "            candidates = []\n",
    "    best_sequence = max(beam, key=lambda x: x[0])[1]\n",
    "    best_sequence = [word for word in best_sequence if word not in ['<S>', '</S>']]\n",
    "    return \" \".join(best_sequence)\n",
    "\n",
    "# --- 7. Generate Text with Beam Search---\n",
    "print(f\"\\n--- Generated Text ---\")\n",
    "# beam_size=1 is same as greedy search as seen above\n",
    "for i in range(5): # Generate a few sentences\n",
    "    generated_sentence = generate_text_with_beam_search(trigram_model, num_words=20, beam_size=5)\n",
    "    print(f\"Generated {i + 1}: {generated_sentence.capitalize()}.\") # Capitalize for better readability\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
